%% REPLACE sXXXXXXX with your student number
\def\studentNumber{s2187249}


%% START of YOUR ANSWERS
%% Add answers to the questions below, by replacing the text inside the brackets {} for \youranswer{ "Text to be replaced with your answer." }. 
%
% Do not delete the commands for adding figures and tables. Instead fill in the missing values with your experiment results, and replace the images with your own respective figures.
%
% You can generally delete the placeholder text, such as for example the text "Question Figure 2 - Replace the images ..." 
%
% There are 19 TEXT QUESTIONS (a few of the short first ones have their answers added to both the Introduction and the Abstract). Replace the text inside the brackets of the command \youranswer with your answer to the question.
%
% There are also 3 "questions" to replace some placeholder FIGURES with your own, and 3 "questions" asking you to fill in the missing entries in the TABLES provided. 
%
% NOTE! that questions are ordered by the order of appearance of their answers in the text, and not by the order you should tackle them. Specifically, you cannot answer Questions 2, 3, and 4 before concluding all of the relevant experiments and analysis. Similarly, you should fill in the TABLES and FIGURES before discussing the results presented there. 
%
% NOTE! If for some reason you do not manage to produce results for some FIGURES and TABLES, then you can get partial marks by discussing your expectations of the results in the relevant TEXT QUESTIONS (for example Question 8 makes use of Table 1 and Figure 2).
%
% Please refer to the coursework specification for more details.

%% - - - - - - - - - - - - TEXT QUESTIONS - - - - - - - - - - - - 

%% Question 1:
\newcommand{\questionOne} {
\youranswer{a condition where the network function too closely fits to the training set and results in poor generalization}
}

%% Question 2:
\newcommand{\questionTwo} {
\youranswer{can generally make the neural network more easily to be overfitting or even make the overfitting more serious.}
}

%% Question 3:
\newcommand{\questionThree} {
\youranswer{although decreasing the keep rate or increasing the L1, L2 penalty cofficient can effectively reduce the generalization gap,
it could also do harm to the model's performance on the validation set, for the reason that those actions can ignore or deemphasize
the information cells carry and cause very serious underftting. We should be careful about the selection of drop rate and penalty cofficient
to prevent such things from happenning.}
}

%% Question 4:
\newcommand{\questionFour} {
\youranswer{the parameters or structures in neural networks all have its own pros and cons regarding to the network performance and overfitting extent. The overfitting can always happen when training neural network. What's important is that we should carefully select these parameters or structures based on their principles to mitigate overfitting and achieves good performance on the validation set. In addition, various experiments are also essential when selecting the best hyper-parameters.}
}

%% Question 5:
\newcommand{\questionFive} {
\youranswer{it has a poor performance on the validation set or test set,
in sharp contrast to its' rather good performance on the training set. In short, it has poor ability for generalization}
}

%% Question 6:
\newcommand{\questionSix} {
\youranswer{Overfitting occurs when the model learns too much about and fits too close to the training set, and then lose the ability to generalize to the unseen data.
This could be identified based on the increasing validation error after some point and the inceasing gap between the performance on validation set and training set.}
}

%% Question 7:
\newcommand{\questionSeven} {
\youranswer{it is about the relationship between epoch and accuracy, containing 2 curves which represent accuracy on training set and test set respectively.
And for Figure 1b, it is about the relationship between epoch and error, containing 2 curves which represent accuracy on training set and test set respectively.
As is the same point in both figures, we can see that the difference on $y$ value between training set and validation set generally increases along the epoch
number when epoch>10. For more details, you can see that the accuracy on validation set nearly reaches a maximum at epoch 10, and then decreases along the epoch, while the accuracy
on training set keeps increasing in the Figure 1a. The similar case also happens in Figure 1b, but the curves move in the opposite direction.
Based on the idea that overfitting shows as the increasing gap between performance on training set and validation set, we can simply infer that
the overfitting actually occur at nearly epoch 10 according to these Figures.
}
}

%% Question 8:
\newcommand{\questionEight} {
\youranswer{the generalization gap increases with the increasing of hidden units in the final epoch in contrast to the slight changes in validation accuracy,
 which indicts that the adding more hidden units may not improve the final performance but make the overfitting more serious in this case. From the Fig \ref{fig:width}, we
 can also observe that actually the best result on validation set of network with width 64 and 128 already reaches at nearly epoch 10. However, after this point,
 their performance starts to drop along the epoch number and the generalization gap also increases. In addition, the wider the network is,
the more rapidly you will find it increases the error on the validation set. But for the network with width 32, we can see that its performance
 on the validation set actually becomes better along the epoch number. In Table \ref{tab:width_exp}, based on the validation accuracy, the wider networks
outperform the narrower one to a small extent, but suffer more generalization gap according to their width.}
}

%% Question 9:
\newcommand{\questionNine} {
\youranswer{The width of the network could affect extent of overfitting in a consistent way. Generally speaking, adding more hidden units could make the network
more likely to be overfitting and make the overfitting more serious. However, in spite of the more serious overfitting conditions for width 64 and 128 in this case,
they finally get a slightly higher accuary than the width 32. So, could we say that a wider network will be a better choice in this case?
Based on the previous knowledge, the wider the network, the stronger fitting ability it will have, it would have a better result than narrow networks if the overfitting does not
happen. But it has a higher chance to get overfitting. This actually matches our experiment since the wider network (64 and 128) does better before overfitting (nearly at epoch 10) in this case.
However, their performance on validation set keeps droping after that. We could expect that its accuracy and error may even be much worse than that of width 32 if we make the epoch number larger.
So, for a wider network, we should carefully choose the epoch number to avoid overfitting, otherwise the overfitting will make the wider networks a bad choice.}
}

%% Question 10:
\newcommand{\questionTen} {
\youranswer{the generalization gap increases with the increasing of hidden layers in the final epoch in contrast to the slight changes in validation accuracy,
 which indicts that the adding more hidden layers may not improve the final performance but make the overfitting more serious in this case. From the Fig. \ref{fig:depth}, we
 can also observe that actually the best result on validation set of networks already reaches at nearly epoch 10. However, after this point,
 their performance starts to drop along the epoch number and the generalization gap also increases. In addition, the deeper the network is,
the more rapidly you will find it increases the error on the validation set.  In Table \ref{tab:depth_exps}, based on the validation accuracy, the deeper networks
outperform the shallower one to a small extent, but suffer more generalization gap according to their depth.}
}

%% Question 11:
\newcommand{\questionEleven} {
\youranswer{According to the observed result, the depth of the network could affect extent of overfitting in a consistent way. Generally speaking, adding more hidden layers could
make the network the overfitting more serious because the generalization gap increases along the depth in this case.
Based on the previous knowledge, the deeper the network is, the stronger fitting ability it will have, it would have a better result than shallower networks if the overfitting does not
happen. But it has a higher chance to get overfitting. However, from the previous Fig. \ref{fig:width}, we already know that the line for depth 1 overfit early epoch 10.
Here we could not draw the conclusion that depth is related to the chance of being overfitting since all curves on the Fig. \ref{fig:depth} start to be overfitting at nearly
the same point. However, we do draw the conclusion that extent of overfitting can be consistently related to number of hidden layer because
the generalization gap increases with the depth increases according to Table \ref{tab:depth_exps}}
}

%% Question 12:
\newcommand{\questionTwelve} {
\youranswer{From the expriments mentioned above, we can draw the conclusion that adding width and adding depth can similar effect. That is, the model will have the better fitting ability, but
suffer more possibility to be overfitting and could make the overfitting worse. In addition, if the overfitting does not occur, wider and deeper neural networks could generally have a better performance
on the validation set than narrower and shallower networks. However, after getting overfitting, they could even have a worse performance than the
the narrower and shallower ones because their performance could drop rapidly along the epoch. Based on the ideas listed above,
    we should carefully tune the width and depth of the networks, otherwise they could easily get overfitting when the epoch number is large.}
}

%% Question 13:
\newcommand{\questionThirteen} {
\youranswer{L1 and L2 regularization mean that we will finally add a term related to the neural network weight $w$ when computing the lost function, with the intention of making
the network not so flexible by putting some constraints on $w$. In particular, the L1 regularization uses first power term to deal with $w$. It adds $\lambda_1\sum_w{\lvert w_i \rvert}$
to the lost function. The  L2 regularization uses quadratic term to deal with $w$. It adds $\lambda_2\sum_w w_i^2 $to the lost function. In the 2 formulas mentioned above, the
parameters $\lambda_1$ and $\lambda_2$ are the hyper-parameter we need to set by ourself according to specific training conditions. These terms mentioned
above will be counted into the final lost  function, which the neural network tries to minimize. In addition, these terms will also take in the part of computing gradients.}
}

%% Question 14:
\newcommand{\questionFourteen} {
\youranswer{Since the L1 regularization and L2 regularization methods both add a term related to the weight $w$, the neural network will try to minimize the new lost function
    (original lost function + regularization term). Because the regularization term involves weight $w$, the neural network then tend not to use the larger $w$ to fit the data
more flexible, but rather to use smaller $w$ to smoothly fit the data, resulting in a better generalization to the unseen data. Thus these regularization methods can alleviate overfitting
to some extent. L1 regularization uses first power term, its gradients would be $\lambda_1 sign_{+-}(w_i)$ for the weight $w_i$, while $2\lambda_2w_i$ for L2 regularization.
As we can see in the two gradients, the L1 regularization punishes the weight $w_i$ in a constant way while the L2 regularization punishes the weight $w_i$ in relationship with $w_i$.
We can image that, after several epochs, the $w_i$  could become 0 under L1 regularization, but may not become 0 under L2 regularization because the gradients is related to  $w_i$ value.
As a result, L1 regularization may set some $w_i$ to 0, whose process could be viewed as feature selection by removing some useless $w_i$. It finally alleviates overfitting by removing some features.
L2 regularization tries to achieve a balance between the original lost function and L2 term. And finally it can also alleviate overfitting by making the fitting line smoother since these
$w_i$ will not be too deviated from 0 under L2 regularization.}
}

%% Question 15:
\newcommand{\questionFifteen} {
\youranswer{We focus on drop out first. As can be seen in Fig. \ref{fig:dropoutrates},
    when the dropout keep rate increases,the performance on the validation set increases in a declining ratio,
    but the generalization gap increases more rapidly. The best performance can be achieved when keep rate = 0.9,
    which achieves 0.847 accuracy on the test set. Generally speaking, if the dropout keep rate is too low,
    few cells are activated and the network can't learn things because of the few information. In contrast,
    without drop out layers, the network could learn too many things and could be overfitting.
    The similar rule also applies on the L1 and L2 regularization methods. In Fig. \ref{fig:weightrates},
    when the penalty coefficient increases, although the generalization gap decreases,
    its performance on the validation first increases and then decreases rapidly. The best performance on validation set
    could be achieved when L1 penalty = 1e-4 or L2 penalty = 1e-3, which also achieves 0.843 and 0.847 accuracy on the
    test set respectively. If the penalty coefficient is too big, the network will try to only minimize the regularization
    term rather than original loss function, which results in poor performance on the validation set. Then we combine several
    hyper-parameters around the best hyper-parameters we found before, to see which might be the best combination.
    As can be seen in Table \ref{tab:hp_search}, Fig. \ref{fig:extra1}, Fig. \ref{fig:extra2}, the best performance on
    validation set achieves by the setting \{lr:5e-4, keep:0.9, l1:1e-4\} as 0.858  and rather low generalization gap as
    0.047. It achieves accuracy as 0.844 on the test set. Other settings have the very similar performance expect the
    setting c7 and c8 for the reason that their penalty coefficient may be too large in this case. These combinations
    tested in this experiment may not be the best one, but it can give us a feeling about how to tune these hyper-parameters
    and how these hyper-parameters affect the performance and overfitting extent for our models.}
}

%% Question 16:
\newcommand{\questionSixteen} {
\youranswer{it has not previously been demonstrated to actually perform model averaging for deep architectures. In addition, the author wants to prove that
rather than using dropout as a slight performance enhancement applied to arbitrary models, the best performance may be obtained by directly
designing a model that enhances dropout’s abilities as a model averaging technique.}
}

%% Question 17:
\newcommand{\questionSeventeen} {
\youranswer{The Dropout is compatible with Maxout. Based on the standard practice, \cite{goodfellow2013maxout} found that the combination of Maxout and Dropout can
make a more robust network and tend to achieve great performance. By principle, Dropout can encourage Maxout to larger linear region. Because the varying Dropout mask
frequently changes linear parts the Maxout can use. However, since the Maxout tries to learn the same thing no matter which cells are dropped, it can finally be
trained to have the characteristic of being stable against the varying mask. Since the Maxout shows as a combination of lines by maximizing serveral linear functions,
    it encourages the Dropout to do accurate model averaging in deeper achitecture, in contrast to the curved activation functions which may cause inaccurate model averaging.
What's more, the combination of them suffer less probability to saturate, suggesting that Maxout
better propagates varying information downward to the lower layers and helps dropout training to better resemble bagging for the lower-layer parameters. }
}

%% Question 18:
\newcommand{\questionEighteen} {
\youranswer{Then the author evaluated the its performance on 4 benchmark datasets(MNIST, CIFAR-10, CIFAR-100, SVHN) and found that the combination
of Maxout and Dropout can achieve rather lower test error compared to other method combinations. Afterwards, the author compared Maxout with rectfiers by
running a large cross-validation experiment and found that the Maxout generally achieves lower validation error. To prove that dropout is indeed performing
model averaging, even in multilayer networks, and that it is more accurate in the case of Maxout, the author conducted experiment on computing the test error
on MINIST classification related to model averaging and KL divergence between model averaging strategies, compared with Tanh activation, which proves his ideas.
Finally, he also set up serveral experiments to the optimization ability for Maxout, and found that Maxout achieved better performance in network with increasing
depth and is less likely to saturate compared with rectifier, based on which the author draws the conclusions that Maxout allows training deeper
networks and better propagating varying information downward to the lower layers. These conclusions mentioned above are all based on actual experiments results, which
are followed by some principle reasoning from the author. Generally speaking, These conclusions are solid and convincing since the author offers both statistics and
corresponding principles.}
}
%% Question 19:
\newcommand{\questionNineteen} {
\youranswer{
Generally speaking, adding hidden units, hidden layers can give the model better fitting ability at the sacrifice of generalization ability. In contrast, decreasing the drop out keep rate or increasing the penalty coefficient can give the model better generalization ability at the sacrifice of fitting ability. Overfitting is the common phenomenon when training the neural network. To deal with overfitting, we should concentrate on the choosing these the network structures and hyper-parameters carefully both by principle and experiments. A balance should be achieved between the fitting ability and the generalization ability, which could finally enable our networks to have a good performance on the test set.
}
}


%% - - - - - - - - - - - - FIGURES - - - - - - - - - - - -

%% Question Figure 2:
\newcommand{\questionFigureTwo} {
\youranswer{Question Figure 2 - Replace the images in Figure 2 with figures depicting the accuracy and error, training and validation curves for your experiments varying the number of hidden units.

\begin{figure}[t]
    \centering
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{figures/acc_curve_width.pdf}
        \caption{accuracy by epoch}
        \label{fig:width_acccurves}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/error_curve_width.pdf}
        \caption{error by epoch}
        \label{fig:width_errorcurves}
    \end{subfigure}
    \caption{Training and validation curves in terms of classification accuracy (a) and cross-entropy error (b) on the EMNIST dataset for different network widths.}
    \label{fig:width}
\end{figure}

}
}

%% Question Figure 3:
\newcommand{\questionFigureThree} {
\youranswer{Question Figure 3 - Replace these images with figures depicting the accuracy and error, training and validation curves for your experiments varying the number of hidden layers.

\begin{figure}[t]
    \centering
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{figures/acc_curve_depth.pdf}
        \caption{accuracy by epoch}
        \label{fig:depth_acccurves}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/error_curve_depth.pdf}
        \caption{error by epoch}
        \label{fig:depth_errorcurves}
    \end{subfigure}
    \caption{Training and validation curves in terms of classification accuracy (a) and cross-entropy error (b) on the EMNIST dataset for different network depths.}
    \label{fig:depth}
\end{figure}

}
}

%% Question Figure 4:
\newcommand{\questionFigureFour} {
\youranswer{Question Figure 4 - Replace these images with figures depicting the Validation Accuracy and Generalisation Gap for each of your experiments varying the Dropout rate, L1/L2 weight penalty, and for the 8 combined experiments (you will have to find a way to best display this information in one subfigure).

\begin{figure*}[t]
    \centering
    \begin{subfigure}{.4\linewidth}
        \includegraphics[width=\linewidth]{figures/metrics_by_dropout_rate.pdf}
        \caption{Metrics by dropout rate}
        \label{fig:dropoutrates}
    \end{subfigure}
    \begin{subfigure}{.4\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/metrics_by_weight_penalty.pdf}
        \caption{Metrics by weight penalty}
        \label{fig:weightrates}
    \end{subfigure}
    \begin{subfigure}{.4\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Combinations with L1.pdf}
        \caption{Extra experiments with L1}
        \label{fig:extra1}
    \end{subfigure}
        \begin{subfigure}{.4\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Combinations with L2.pdf}
        \caption{Extra experiments with L2}
        \label{fig:extra2}
    \end{subfigure}
    \caption{Hyperparameter search for every method and combinations}
    \label{fig:hp_search}
\end{figure*}

}
}

%% - - - - - - - - - - - - TABLES - - - - - - - - - - - -

%% Question Table 1:
\newcommand{\questionTableOne} {
\youranswer{
Question Table 1 - Fill in Table 1 with the results from your experiments varying the number of hidden units.

\begin{table}[t]
    \centering
    \begin{tabular}{c|cc}
    \toprule
        \# hidden units & val. acc. & generalization gap \\
    \midrule
         32            &     0.785    &             0.148        \\
         64            &     0.811     &            0.339       \\
         128           &     0.808     &            0.782        \\
    \bottomrule
    \end{tabular}
    \caption{Validation accuracy (\%) and generalization gap (in terms of cross-entropy error) for varying network widths on the EMNIST dataset.}
    \label{tab:width_exp}
\end{table}
}
}

%% Question Table 2:
\newcommand{\questionTableTwo} {
\youranswer{
Question Table 2 - Fill in Table 2 with the results from your experiments varying the number of hidden layers.

\begin{table}[t]
    \centering
    \begin{tabular}{c|cc}
    \toprule
        \# hidden layers & val. acc. & generalization gap \\
    \midrule
         1               &      0.805      &        0.798             \\
         2               &      0.815      &        1.429            \\
         3               &      0.821      &        1.565           \\
    \bottomrule
    \end{tabular}
    \caption{Validation accuracy (\%) and generalization gap (in terms of cross-entropy error) for varying network depths on the EMNIST dataset.}
    \label{tab:depth_exps}
\end{table}
}
}

%% Question Table 3:
\newcommand{\questionTableThree} {
\youranswer{
Question Table 3 - Fill in Table 3 with the results from your experiments varying the hyperparameter values for each of L1 regularisation, L2 regularisation, and Dropout (use the values shown on the table) as well as the results for your experiments combining L1/L2 and Dropout (you will have to pick what combinations of hyperparameter values to test for the combined experiments; each of the combined experiments will need to use Dropout and either L1 or L2 regularisation; run an experiment for each of 8 different combinations). Use \textit{italics} to print the best result per criterion for each set of experiments, and \textbf{bold} for the overall best result per criterion.

\begin{table*}[t]
    \centering
    \begin{tabular}{c|c|cc}
    \toprule
        Model    &  Hyperparameter value(s) & Validation accuracy & Generalization gap \\
    \midrule
    \midrule
        Baseline &       lr=0.001, hidden layers = 3, hidden units = 128                &           82.5 \%           &        1.30           \\
    \midrule
        \multirow{5}*{Dropout}
                 & 0.1                   &           0.107          &         0.006           \\
                 & 0.3                   &           0.685          &         0.044           \\
                 & 0.5                   &            0.797           &         0.073           \\
                 & 0.7                   &           0.840          &         0.118          \\
                 & 0.9                   &           0.849           &         0.276          \\
    \midrule
        \multirow{5}*{L1 penalty}
                 & 1e-5                   &         0.836            &        0.783           \\
                 & 1e-4                   &         0.851            &        0.096           \\
                 & 1e-3                   &         0.764            &        0.019          \\
                 & 1e-2                   &         0.020           &         5.990e-4          \\
                 & 1e-1                   &         0.020           &         3.998e-4          \\
    \midrule
        \multirow{5}*{L2 penalty}
                 & 1e-5                   &         0.820          &         1.273          \\
                 & 1e-4                   &         0.835            &       0.517            \\
                 & 1e-3                   &         0.853            &       0.065           \\
                 & 1e-2                   &         0.755           &        0.008           \\
                 & 1e-1                   &         0.020           &        5.241e-4           \\
    \midrule
        \multirow{8}*{Combined}
                 & c1 \{lr:1e-3, keep:0.9, l1:1e-4\}                  &        0.853             &         0.041          \\
                 & c2 \{lr:1e-3, keep:0.9, l2:1e-3\}                   &       0.839              &        0.037           \\
                 & c3 \{lr:5e-4, keep:0.9, l1:1e-4\}                   &        0.858             &        0.047           \\
                 & c4 \{lr:5e-4, keep:0.9, l2:1e-3\}                   &        0.848             &        0.046           \\
                 & c5 \{lr:1e-3, keep:0.95, l1:1e-4\}                   &       0.858              &       0.055            \\
                 & c6 \{lr:1e-3, keep:0.95, l2:1e-3\}                   &       0.852              &       0.046            \\
                 & c7 \{lr:1e-3, keep:0.9, l1:5e-3\}                   &        0.020             &        5.194e-4           \\
                 & c8 \{lr:1e-3, keep:0.9, l2:5e-2\}                   &        0.062             &        1.361e-4           \\
    \bottomrule
    \end{tabular}
    \caption{Results of all hyperparameter search experiments. \emph{italics} indicate the best results per series and \textbf{bold} indicate the best overall}
    \label{tab:hp_search}
\end{table*}
}
}

\newcommand{\combinationTable} {
\setlength{\tabcolsep}{4.5mm}{
\begin{table}[t]
    \centering
    \begin{tabular}{c|c|c|c|c}
    \toprule
        \ name & lr & keep & L1 & L2 \\
    \midrule
         c1               &      1e-3      &        0.9 & 1e-4 &         \\
         c2               &      1e-3      &        0.9 &   & 1e-3           \\
         c3               &      5e-4      &        0.9   & 1e-4 &        \\
         c4               &      5e-4      &       0.9    & & 1e-3        \\
         c5               &      1e-3      &       0.95   &  1e-4 &        \\
         c6               &      1e-3      &        0.95   &  & 1e-3        \\
         c7               &      1e-3      &        0.9     & 5e-3 &        \\
         c8               &      1e-3      &        0.9     & & 5e-2     \\

    \bottomrule
    \end{tabular}
    \caption{Combinations for various hyper-parameters }
    \label{tab:combine}
\end{table}
}
}

%% END of YOUR ANSWERS